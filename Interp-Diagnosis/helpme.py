# -*- coding: utf-8 -*-
"""Projectcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tU3EbOtWCHv5z-MSJs46KuTzPoGlrDnf
"""

torch_version_suffix = "+cu110"
import os
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from PIL import Image
from torch import nn
from torch.optim import lr_scheduler
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Resize

import clip
from clip.simple_tokenizer import SimpleTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

_tokenizer = SimpleTokenizer()


def tokenize(texts, context_length: int = 77) -> torch.LongTensor:
    if isinstance(texts, str):
        texts = [texts]

    sot_token = _tokenizer.encoder["<|startoftext|>"]
    eot_token = _tokenizer.encoder["<|endoftext|>"]
    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]
    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)

    for i, tokens in enumerate(all_tokens):
        n = min(len(tokens), context_length)
        result[i, :n] = torch.tensor(tokens)[:n]
        if len(tokens) > context_length:
            result[i, -1] = tokens[-1]

    return result


class RollingMean():
    def __init__(self):
        self.n = 0
        self.mean = 0

    def update(self, value):
        self.mean = (self.mean * self.n + value) / (self.n + 1)
        self.n += 1

    def result(self):
        return self.mean


class MyDataset(Dataset):
    def __init__(self, df, images_path, transform):
        # super().__init__()
        self.transform = transform
        self.df = df
        self.images_path = images_path
        self.classes = ['no', 'pre-proliferative', 'proliferative']
        data = {}
        for k, v in df.groupby('level'):
            if len(v) < 1000:
                data[k] = v
            else:
                data[k] = v.iloc[:len(v), :]
        self.data = pd.concat([data[i] for i in range(3) if i in data], axis=0)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_name, text = self.df.iloc[idx, :].values
        image = self.transform(Image.open(os.path.join(self.images_path, img_name + '.jpeg')))
        return image, text


# class Metric():
#     def __init__(self, CLASSES):
#         self.CLASSES = CLASSES
#         self.top1recall = {}
#         self.top1precision = {}

#     def clear(self):
#         for i in range(len(self.CLASSES)):
#             self.top1recall[i] = []
#             self.top1precision[i] = []
#     def update(self, similarity, labels):
#         #similarity:Nx7 labels:N
#         for i, label in enumerate(labels):
#             tar = int(label)
#             pre = similarity[i].topk(1).indices
#             if pre == label:
#                 self.top1precision[int(pre)].append(1)
#                 self.top1recall[tar].append(1)
#             else:
#                 self.top1precision[int(pre)].append(0)
#                 self.top1recall[tar].append(0)
#     def report(self):
#         table_header = ["class","Precision","Recall","F1"]
#         table_data = []
#         for i, cls in enumerate(self.CLASSES):
#             recall = np.mean(self.top1recall[i])
#             precision = np.mean(self.top1precision[i])
#             f1 = 2 * (precision * recall) / (precision + recall)
#             table_data.append((cls, str(precision)[:4], str(recall)[:4], str(f1)[:4]))

#         print(tabulate(table_data, headers=table_header, tablefmt='grid'))

def train(n_epochs, batch_size, learning_rate):
    # Load CLIP
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load('ViT-B/32', device)
    model.train()

    # load train data
    train_images_path = Path('/Users/shuumichi/Desktop/CLIP-main/data/train/train0')

    df_train = pd.read_csv('/Users/shuumichi/Desktop/CLIP-main/data/train0.csv')

    dstrain = MyDataset(df_train, train_images_path, preprocess)
    # print(dstrain.data[:5])
    dltrain = DataLoader(dstrain, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False)
    texts = torch.cat([clip.tokenize(f"{c} retinopathy") for c in dstrain.classes]).to(device)

    # optim = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=1e-2)
    textifier = nn.Sequential(nn.Linear(1, 8),
                               nn.ReLU(),
                               nn.Linear(8, 3),
                               nn.Sigmoid())

    textifier.train()
    optim = torch.optim.Adam(textifier.parameters(), lr=learning_rate, betas=(0.9, 0.99))
    scheduler = lr_scheduler.StepLR(optim, step_size=20, gamma=0.3)

    loss_img = nn.CrossEntropyLoss(weight=torch.FloatTensor([0.1, 1.5, 0.5])).to(device)

    # finetune
    best_loss = np.inf
    for i in range(n_epochs):
        batch_loss = RollingMean()
        # cnt = 0
        # for images, labels in tqdm(dltrain):
        for images, labels in dltrain:

            optim.zero_grad()
            image_features = model.encode_image(images.to(device))
            texts= model.encode_text(labels)
            predictions=textifier(texts)

            loss = loss_img(predictions, image_features)

            loss.backward()
            optim.step()
            # Update metric
            batch_loss.update(loss.item())

            # if cnt % 100 == 0:
            #     print('epoch: {}, batch count: {}, loss: {}'.format(i, cnt, batch_loss.result()))
            # cnt += 1

        scheduler.step()
        loss_mean = batch_loss.result()
        print('epoch: {}, total loss: {}'.format(i, loss_mean))

        if loss_mean < best_loss:
            best_loss = loss_mean
            torch.save({
                'epoch': i,
                'model_state_dict': textifier.state_dict(),
                'optimizer_state_dict': optim.state_dict(),
                'loss': loss_mean,
            }, f"textifier.pt")
        # print(classifier.state_dict())


# model, preprocess = clip.load("ViT-B/32",device=device,jit=False)
# checkpoint = torch.load("model.pt")
# model.load_state_dict(checkpoint['model_state_dict'])

def main():

 # Load CLIP
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load('ViT-B/32', device)
    model.train()

    # load train data
    train_images_path = Path('/Users/shuumichi/Desktop/CLIP-main/data/train/test')

    df_train = pd.read_csv('/Users/shuumichi/Desktop/CLIP-main/data/test.csv')

    dstrain = MyDataset(df_train, train_images_path, preprocess)
    dltrain = DataLoader(dstrain, batch_size=8, shuffle=False, num_workers=4, drop_last=False)
    texts = torch.cat([clip.tokenize(f"{dstrain} retinopathy") for dstrain in dstrain.classes]).to(device)

if __name__ == "__main__":
    main()